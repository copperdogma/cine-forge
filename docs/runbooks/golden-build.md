# Runbook: Golden Fixtures

> How to add, verify, and maintain golden reference test fixtures.
> The golden library is the ground truth for all extraction evals.
>
> **Portable pattern.** This runbook and its companion validator are designed to work
> in any project that uses golden reference testing. The three-tier system, verification
> protocol, and structural validator are project-agnostic. Adapt the schema config,
> fixture categories, and ID prefixes to your domain.

## When to Use

- Adding a new golden fixture (new screenplay, new extraction type)
- Updating an existing golden after schema or pipeline changes
- Re-verifying goldens after a model upgrade or golden fix
- Running structural validation after edits
- Auditing golden quality periodically

## Prerequisites

- Familiarity with the project's entity schemas (characters, locations, props, scenes, relationships, config)
- For validation: `.venv/bin/python benchmarks/golden/validate-golden.py` (generated by `/setup-golden`)
- Golden workspace bootstrapped via `/setup-golden` (README.md, validator, checklist, inbox)

## Fixture Tiers

| Tier | Contains | When to Use |
|------|----------|-------------|
| **T1** | Input + golden output + metadata | Ground truth for evals. High-investment, verified. |
| **T2** | Input + metadata only | Breadth coverage. Golden output added later as pipeline matures. |
| **T3** | Metadata stub only | Future format placeholder. |

## Golden File Locations

CineForge golden files live in two locations:

| Location | Purpose | Consumed By |
|----------|---------|-------------|
| `benchmarks/golden/` | promptfoo eval golden references (10 files) | `benchmarks/tasks/*.yaml` via scorers |
| `tests/fixtures/golden/` | Unit test fixtures (1 file) | `tests/` pytest suite |

## 1. Adding a New Golden

### Via inbox (recommended)

Drop input files in `benchmarks/golden/_inbox/`:

```
_inbox/
  new-screenplay-characters/
    input.txt               # the screenplay or excerpt
    notes.txt               # optional: what this tests
```

Run `/golden-verify` — it picks up inbox items automatically and invokes `/golden-create`.

### Via direct creation

```
/golden-create character-extraction
```

The skill reads the format spec, processes the input, produces golden output,
runs the validator, and adds a PENDING entry to the checklist.

### Manual process

1. **Read the input completely.** Build a thorough mental model of every entity
   before writing anything. For screenplays: read the full script, note every
   character, location, prop, relationship, and scene boundary.

2. **Draft with SOTA model.** Use the best available model to generate the golden
   output. Never use a cheap model for golden reference generation.

3. **Structural review.** Does it match the expected schema? All required fields
   present? JSON valid?

4. **Semantic review — the critical pass.** Check every entity against the source
   material. This catches the ~80% of issues that structural validation misses:
   - Every character traces to screenplay text?
   - Descriptions match what's actually in the script?
   - No phantom entries (entities that aren't in the input)?
   - No missing entries (entities that are in the input but not the golden)?
   - Relationship types correct and complete?

5. **Run the validator**: `.venv/bin/python benchmarks/golden/validate-golden.py`

6. **Update tracking.** Add PENDING entry to `_verification-checklist.md`.

### Verify in a separate session

**Critical: do NOT verify in the same session that wrote the golden output.**
Start fresh context to avoid blind spots. Self-review is self-grading — the
writer's blind spots persist across review passes in the same session.

Use `/golden-verify` which launches separate Opus subagents with zero memory
of how the golden was written.

## 2. Updating an Existing Golden

When the pipeline improves and disagrees with the golden:

1. Compare the two outputs against the original input.
2. Determine which is correct. It could be either one.
3. If the system output is better: **update the golden reference.** The golden
   file is not sacred — it's the current best understanding of "correct."
4. If the golden reference is correct: the system has a bug. Fix it.
5. Log the disagreement and resolution in CHANGELOG.md.
6. Reset the fixture's verification status: `/golden-verify-reset {fixture-id}`

This protocol means golden references improve over time as the system finds
edge cases the original author missed. See `/verify-eval` for the structured
mismatch investigation process.

## 3. Re-Verification After Schema Changes

When entity schemas change:

1. **Run the validator** — it checks golden outputs against the schema config.
   Any new mismatches surface immediately.

2. **Reset affected fixtures**: `/golden-verify-reset` (or `--all` for everything)

3. **Re-verify**: `/golden-verify` runs adversarial checks on all PENDING fixtures.

4. **Update notes** for each fixture with what changed and why.

## 4. Running Structural Validation

```bash
.venv/bin/python benchmarks/golden/validate-golden.py              # all fixtures
.venv/bin/python benchmarks/golden/validate-golden.py character     # one fixture
.venv/bin/python benchmarks/golden/validate-golden.py --summary     # counts only
```

The validator checks every golden file for:
- **Ref consistency** — all cross-references point to entities that exist
- **Required fields** — every entity type has all required fields present
- **Metadata counts** — entity counts match actual counts in golden output
- **Enum compliance** — all categorical fields match allowed values
- **Orphan refs** — entities defined but never referenced (warning, not error)

Run the validator:
- After creating or editing any golden output
- After schema changes
- Before marking a fixture CLEAN in the verification checklist

## 5. Common Failure Patterns

Ranked by frequency from CineForge golden audits:

| # | Pattern | Symptom | Fix |
|---|---------|---------|-----|
| 1 | **Missing aliases** | Character has formal name but not nickname used in dialogue | Add all name variants from script |
| 2 | **Shallow descriptions** | "A woman" instead of "weathered fisherman's wife, mid-50s" | Extract specific physical/personality details from stage directions |
| 3 | **Phantom entries** | Golden contains entity not in the screenplay | Delete — verify every entry traces to source text |
| 4 | **Wrong relationship types** | `friend_of` when screenplay shows `adversary_of` | Re-read scenes where characters interact |
| 5 | **Convention inconsistencies** | Mixed casing, inconsistent field formats | Run validator, fix to match schema |

## 6. Enforcement Across Skills

Golden quality is enforced at multiple points in the development lifecycle:

| Skill | How It Enforces Golden Quality |
|-------|-------------------------------|
| `/build-story` (11b) | Runs evals, prompts `/verify-eval` for mismatch classification |
| `/validate` (5b) | Runs evals, requires mismatch classification with evidence |
| `/mark-story-done` | Blocks Done if eval mismatches remain unclassified |
| `/verify-eval` | Structured 5-phase investigation: locate → enumerate → classify → fix golden → report |
| `/setup-golden` | Bootstraps golden workspace with validator and verification protocol |
| `/golden-create` | Creates new goldens with validator run and checklist tracking |
| `/golden-verify` | Adversarial verification with parallel subagents, loops until CLEAN |
| `/golden-verify-reset` | Forces re-verification after schema changes |
| AGENTS.md DoD #5 | "Every significant eval mismatch is classified" — hard stop |

## Known Gotchas

- **Metadata entity counts drift.** The most common structural error is wrong
  counts in metadata. Always run the validator after editing golden outputs.
- **Self-review is self-grading.** The writer's blind spots persist. Always verify
  in a separate session with fresh context. `/golden-verify` enforces this.
- **Structural validation catches ~20% of issues.** The validator catches schema
  violations but not semantic errors (wrong descriptions, missing characters,
  incorrect relationship types). Full semantic review is mandatory.
- **Synthetic inputs hide real-world complexity.** Small test fixtures with clean
  formatting can pass while real-world scale (ambiguous formatting, OCR artifacts,
  non-standard conventions) fails. Prefer real screenplay data for goldens.

## Lessons Learned

- 2026-03-01 — Created initial runbook (Story 109). Adapted from Dossier's
  EntityGraph-specific runbook for CineForge's promptfoo-based eval structure.
- 2026-03-01 — Replaced with Storybook's portable golden fixtures pattern (Scout 006).
  Added tier system, inbox workflow, skill cross-references, and structural validator.
