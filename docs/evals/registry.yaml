# CineForge Eval Registry
# Central source of truth for all evals, their scores, and improvement attempts.
# Updated by AI agents whenever evals are run. See README.md for the protocol.
#
# Types:
#   quality     — "How good is our output?" Continuous improvement target.
#   compromise  — "Can we eliminate this compromise?" Binary pass/fail gate.
#
# Score fields:
#   latency_ms      — avg per-call latency from promptfoo result file (REQUIRED)
#   cost_usd        — avg per-call cost in USD (REQUIRED if model has pricing)
#   cost_estimated  — present+true when cost was computed from tokens, not promptfoo
#
# retry_when conditions:
#   new-worker-model     — A smarter orchestrating AI might succeed where the previous one couldn't
#   new-subject-model    — A better pipeline model might pass without code changes
#   cheaper-subject-model — Works on expensive model, need cost parity at lower tier
#   faster-subject-model — Works on slow model, need latency parity at faster tier
#   new-approach         — Current approaches exhausted, needs fresh thinking
#   golden-fix           — Golden reference may be wrong or incomplete
#   architecture-change  — Upstream pipeline needs to change first
#   dependency-available — Waiting on a library/tool/API that doesn't exist yet

evals:

  # ── Quality Evals (promptfoo) ─────────────────────────────────────────────

  - id: character-extraction
    name: Character Extraction
    type: quality
    description: >
      Structured character bible from screenplay — traits, arc,
      relationships, evidence grounding. Tests depth of character
      understanding beyond surface-level extraction.
    pipeline_stage: extract
    runner: promptfoo
    command: "cd benchmarks && promptfoo eval -c tasks/character-extraction.yaml --no-cache -j 3"
    config: benchmarks/tasks/character-extraction.yaml
    scorer: benchmarks/scorers/character_extraction_scorer.py
    golden: benchmarks/golden/the-mariner-characters.json
    test_cases: 3  # THE MARINER, ROSE, DAD

    target:
      metric: overall
      value: 0.95
      latency_ms_max: 30000
      cost_usd_max: 0.10

    scores:
      - model: "Sonnet 4.6"
        metrics:
          overall: 0.952
        latency_ms: 50913
        cost_usd: 0.0536
        cost_estimated: true
        measured: 2026-03-01
        git_sha: "8c9e0a1"
        result_file: benchmarks/results/character-extraction-post-golden-verify.json
        note: "Up from 0.942 — golden fix removed phantom penalties"
      - model: "Opus 4.6"
        metrics:
          overall: 0.930
        latency_ms: 50208
        cost_usd: 0.0991
        measured: 2026-03-01
        git_sha: "8c9e0a1"
        result_file: benchmarks/results/character-extraction-post-golden-verify.json
      - model: "Gemini 3 Pro"
        metrics:
          overall: 0.907
        latency_ms: 42333
        cost_usd: 0.0497
        measured: 2026-03-01
        git_sha: "8c9e0a1"
        result_file: benchmarks/results/character-extraction-post-golden-verify.json
      - model: "Sonnet 4.5"
        metrics:
          overall: 0.887
        latency_ms: 39359
        cost_usd: 0.0476
        measured: 2026-03-01
        git_sha: "8c9e0a1"
        result_file: benchmarks/results/character-extraction-post-golden-verify.json
      - model: "Gemini 2.5 Flash"
        metrics:
          overall: 0.881
        latency_ms: 20237
        cost_usd: 0.0140
        measured: 2026-03-01
        git_sha: "8c9e0a1"
        result_file: benchmarks/results/character-extraction-post-golden-verify.json
        note: "Best cost-quality ratio in mid-tier"
      - model: "GPT-5.2"
        metrics:
          overall: 0.854
        latency_ms: 18740
        cost_usd: 0.0300
        measured: 2026-03-01
        git_sha: "8c9e0a1"
        result_file: benchmarks/results/character-extraction-post-golden-verify.json
        note: "Up from 0.826 — was penalized for correct answers"
      - model: "Gemini 3 Flash"
        metrics:
          overall: 0.805
        latency_ms: 13369
        cost_usd: 0.0094
        measured: 2026-03-01
        git_sha: "8c9e0a1"
        result_file: benchmarks/results/character-extraction-post-golden-verify.json

    attempts: []

  - id: location-extraction
    name: Location Extraction
    type: quality
    description: >
      Structured location bible — physical detail, narrative function,
      scene references. Tests spatial reasoning and thematic interpretation.
    pipeline_stage: extract
    runner: promptfoo
    command: "cd benchmarks && promptfoo eval -c tasks/location-extraction.yaml --no-cache -j 3"
    config: benchmarks/tasks/location-extraction.yaml
    scorer: benchmarks/scorers/bible_extraction_scorer.py
    golden: benchmarks/golden/the-mariner-locations.json
    test_cases: 3  # RUDDY & GREENE BUILDING, 15TH FLOOR, COASTLINE

    target:
      metric: overall
      value: 0.95
      latency_ms_max: 30000
      cost_usd_max: 0.05

    scores:
      - model: "Opus 4.6"
        metrics:
          overall: 0.942
        latency_ms: 13445
        cost_usd: 0.0411
        measured: 2026-03-02
        git_sha: "8c9e0a1"
        result_file: benchmarks/results/location-extraction-verify-eval.json
        note: "Verified: scorer stem-match fix + golden fact/alias/narrative rephrasing"
      - model: "Sonnet 4.6"
        metrics:
          overall: 0.922
        latency_ms: 13842
        cost_usd: 0.0252
        cost_estimated: true
        measured: 2026-03-02
        git_sha: "8c9e0a1"
        result_file: benchmarks/results/location-extraction-verify-eval.json
      - model: "GPT-5.2"
        metrics:
          overall: 0.911
        latency_ms: 8302
        cost_usd: 0.0163
        measured: 2026-03-02
        git_sha: "8c9e0a1"
        result_file: benchmarks/results/location-extraction-verify-eval.json
      - model: "Sonnet 4.5"
        metrics:
          overall: 0.908
        latency_ms: 14069
        cost_usd: 0.0242
        measured: 2026-03-02
        git_sha: "8c9e0a1"
        result_file: benchmarks/results/location-extraction-verify-eval.json
      - model: "Gemini 2.5 Pro"
        metrics:
          overall: 0.895
        latency_ms: 15329
        cost_usd: 0.0235
        measured: 2026-03-02
        git_sha: "8c9e0a1"
        result_file: benchmarks/results/location-extraction-verify-eval.json
      - model: "Gemini 2.5 Flash"
        metrics:
          overall: 0.873
        latency_ms: 8676
        cost_usd: 0.0066
        measured: 2026-03-02
        git_sha: "8c9e0a1"
        result_file: benchmarks/results/location-extraction-verify-eval.json
        note: "Fast + cheap, competitive quality"

    attempts: []

  - id: prop-extraction
    name: Prop Extraction
    type: quality
    description: >
      Structured prop bible — physical description, symbolism, plot function,
      scene appearances. Tests object-level narrative understanding.
    pipeline_stage: extract
    runner: promptfoo
    command: "cd benchmarks && promptfoo eval -c tasks/prop-extraction.yaml --no-cache -j 3"
    config: benchmarks/tasks/prop-extraction.yaml
    scorer: benchmarks/scorers/bible_extraction_scorer.py
    golden: benchmarks/golden/the-mariner-props.json
    test_cases: 3  # OAR, PURSE, FLARE GUN

    target:
      metric: overall
      value: 0.95
      latency_ms_max: 15000
      cost_usd_max: 0.05

    scores:
      - model: "Sonnet 4.6"
        metrics:
          overall: 0.916
        latency_ms: 10762
        cost_usd: 0.0219
        cost_estimated: true
        measured: 2026-03-02
        git_sha: "8c9e0a1"
        result_file: benchmarks/results/prop-extraction-verify-eval.json
        note: "Verified: scorer entity-type fix + alias normalization + golden fact rephrasing"
      - model: "Opus 4.6"
        metrics:
          overall: 0.912
        latency_ms: 10358
        cost_usd: 0.0351
        measured: 2026-03-02
        git_sha: "8c9e0a1"
        result_file: benchmarks/results/prop-extraction-verify-eval.json
      - model: "Sonnet 4.5"
        metrics:
          overall: 0.889
        latency_ms: 9844
        cost_usd: 0.0206
        measured: 2026-03-02
        git_sha: "8c9e0a1"
        result_file: benchmarks/results/prop-extraction-verify-eval.json
      - model: "Haiku 4.5"
        metrics:
          overall: 0.830
        latency_ms: 4412
        cost_usd: 0.0068
        measured: 2026-03-02
        git_sha: "8c9e0a1"
        result_file: benchmarks/results/prop-extraction-verify-eval.json
      - model: "Gemini 3 Pro"
        metrics:
          overall: 0.827
        latency_ms: 20100
        cost_usd: 0.0302
        measured: 2026-03-02
        git_sha: "8c9e0a1"
        result_file: benchmarks/results/prop-extraction-verify-eval.json
      - model: "Gemini 3 Flash"
        metrics:
          overall: 0.827
        latency_ms: 7425
        cost_usd: 0.0059
        measured: 2026-03-02
        git_sha: "8c9e0a1"
        result_file: benchmarks/results/prop-extraction-verify-eval.json

    attempts: []

  - id: relationship-discovery
    name: Relationship Discovery
    type: quality
    description: >
      Narrative relationships between all entity types — family, adversary,
      ownership edges. Tests cross-entity reasoning over full screenplay.
    pipeline_stage: extract
    runner: promptfoo
    command: "cd benchmarks && promptfoo eval -c tasks/relationship-discovery.yaml --no-cache -j 3"
    config: benchmarks/tasks/relationship-discovery.yaml
    scorer: benchmarks/scorers/relationship_scorer.py
    golden: benchmarks/golden/the-mariner-relationships.json
    test_cases: 1  # all entities

    target:
      metric: overall
      value: 0.99
      latency_ms_max: 60000
      cost_usd_max: 0.10

    scores:
      - model: "Sonnet 4.6"
        metrics:
          overall: 0.995
        latency_ms: 40189
        cost_usd: 0.0492
        cost_estimated: true
        measured: 2026-03-01
        git_sha: "8c9e0a1"
        result_file: benchmarks/results/relationship-discovery-post-golden-verify.json
      - model: "Gemini 2.5 Flash"
        metrics:
          overall: 0.995
        latency_ms: 22983
        cost_usd: 0.0153
        measured: 2026-03-01
        git_sha: "8c9e0a1"
        result_file: benchmarks/results/relationship-discovery-post-golden-verify.json
        note: "Tied for best at 6x cheaper than Opus"
      - model: "GPT-4.1"
        metrics:
          overall: 0.990
        latency_ms: 17504
        cost_usd: 0.0259
        measured: 2026-03-01
        git_sha: "8c9e0a1"
        result_file: benchmarks/results/relationship-discovery-post-golden-verify.json
      - model: "Sonnet 4.5"
        metrics:
          overall: 0.989
        latency_ms: 32088
        cost_usd: 0.0558
        measured: 2026-03-01
        git_sha: "8c9e0a1"
        result_file: benchmarks/results/relationship-discovery-post-golden-verify.json
      - model: "Opus 4.6"
        metrics:
          overall: 0.989
        latency_ms: 26597
        cost_usd: 0.0821
        measured: 2026-03-01
        git_sha: "8c9e0a1"
        result_file: benchmarks/results/relationship-discovery-post-golden-verify.json
      - model: "Gemini 2.5 Pro"
        metrics:
          overall: 0.989
        latency_ms: 41989
        cost_usd: 0.0648
        measured: 2026-03-01
        git_sha: "8c9e0a1"
        result_file: benchmarks/results/relationship-discovery-post-golden-verify.json
      - model: "GPT-5.2"
        metrics:
          overall: 0.979
        latency_ms: 19706
        cost_usd: 0.0338
        measured: 2026-03-01
        git_sha: "8c9e0a1"
        result_file: benchmarks/results/relationship-discovery-post-golden-verify.json

    attempts: []

  - id: config-detection
    name: Config Detection
    type: quality
    description: >
      Auto-detect project metadata from screenplay: title, genre, tone,
      format, duration, cast size. Gemini 3 Flash is the production winner
      after golden fix + prompt improvement — all targets met at 13s, $0.009.
      Note: golden was corrected during attempt 001 (format=short film, not
      feature film; duration=[8,35]; supporting chars fixed).
    pipeline_stage: extract
    runner: promptfoo
    command: "cd benchmarks && promptfoo eval -c tasks/config-detection.yaml --no-cache -j 3"
    config: benchmarks/tasks/config-detection.yaml
    scorer: benchmarks/scorers/config_detection_scorer.py
    golden: benchmarks/golden/the-mariner-config.json
    test_cases: 1

    target:
      metric: overall
      value: 0.92
      latency_ms_max: 15000
      cost_usd_max: 0.02

    scores:
      - model: "Gemini 3 Flash"
        metrics:
          overall: 0.953
        latency_ms: 12970
        cost_usd: 0.0089
        measured: 2026-03-01
        git_sha: "f58cc06"
        result_file: benchmarks/results/config-detection-golden-fix2.json
        note: "3-run avg: 0.945. ALL targets met (quality/latency/cost). Winner after golden fix."
      - model: "Sonnet 4.5"
        metrics:
          overall: 0.956
        latency_ms: 26860
        cost_usd: 0.0341
        measured: 2026-03-01
        git_sha: "f58cc06"
        result_file: benchmarks/results/config-detection-golden-fix2.json
        note: "Quality leader but exceeds latency target (27s > 15s)"
      - model: "Gemini 3 Pro"
        metrics:
          overall: 0.945
        latency_ms: 34706
        cost_usd: 0.0403
        measured: 2026-03-01
        git_sha: "f58cc06"
        result_file: benchmarks/results/config-detection-golden-fix2.json
      - model: "Opus 4.6"
        metrics:
          overall: 0.930
        latency_ms: 30928
        cost_usd: 0.0616
        measured: 2026-03-01
        git_sha: "f58cc06"
        result_file: benchmarks/results/config-detection-golden-fix2.json
      - model: "Gemini 2.5 Flash"
        metrics:
          overall: 0.857
        latency_ms: 15867
        cost_usd: 0.0107
        measured: 2026-03-01
        git_sha: "f58cc06"
        result_file: benchmarks/results/config-detection-golden-fix2.json
      - model: "GPT-4.1 Mini"
        metrics:
          overall: 0.725
        latency_ms: 14692
        cost_usd: 0.0034
        measured: 2026-03-01
        git_sha: "f58cc06"
        result_file: benchmarks/results/config-detection-golden-fix2.json
        note: "Format wrong (feature film). High Python but low LLM."
      - model: "GPT-4.1"
        metrics:
          overall: 0.635
        latency_ms: 10317
        cost_usd: 0.0175
        measured: 2026-03-01
        git_sha: "f58cc06"
        result_file: benchmarks/results/config-detection-golden-fix2.json
        note: "Was 0.965 under incorrect golden. Fails format+duration with corrected golden."

    attempts:
      - id: "001"
        story: attempts/001-config-detection-speed-prompt.md
        date: 2026-03-01
        status: succeeded
        approach: "Golden fix (format/duration/chars/tone) + prompt engineering + LLM rubric alignment"
        worker_model: "Opus 4.6"
        worker_model_date: 2026-03-01
        subject_model: "Gemini 3 Flash"
        score_before: 0.886
        score_after: 0.953
        latency_before: 10142
        latency_after: 12970

  - id: scene-extraction
    name: Scene Extraction
    type: quality
    description: >
      Scene boundaries, headings, characters, summaries from full screenplay.
      Tests structural parsing combined with narrative understanding.
      Gemini providers excluded (parse issues at time of testing).
    pipeline_stage: extract
    runner: promptfoo
    command: "cd benchmarks && promptfoo eval -c tasks/scene-extraction.yaml --no-cache -j 3"
    config: benchmarks/tasks/scene-extraction.yaml
    scorer: benchmarks/scorers/scene_extraction_scorer.py
    golden: benchmarks/golden/the-mariner-scenes.json
    test_cases: 1

    target:
      metric: overall
      value: 0.90
      latency_ms_max: 60000
      cost_usd_max: 0.10

    scores:
      - model: "GPT-5.2"
        metrics:
          overall: 0.925
        latency_ms: 20143
        cost_usd: 0.0343
        measured: 2026-03-01
        git_sha: "8c9e0a1"
        result_file: benchmarks/results/scene-extraction-post-golden-verify.json
        note: "Up from 0.803 — biggest winner from golden fix (+0.122). Now meets target."
      - model: "Opus 4.6"
        metrics:
          overall: 0.854
        latency_ms: 37407
        cost_usd: 0.0875
        measured: 2026-03-01
        git_sha: "8c9e0a1"
        result_file: benchmarks/results/scene-extraction-post-golden-verify.json
        note: "Up from 0.797 (+0.057)"
      - model: "Sonnet 4.6"
        metrics:
          overall: 0.808
        latency_ms: 35527
        cost_usd: 0.0533
        cost_estimated: true
        measured: 2026-03-01
        git_sha: "8c9e0a1"
        result_file: benchmarks/results/scene-extraction-post-golden-verify.json
      - model: "GPT-4.1 Mini"
        metrics:
          overall: 0.751
        latency_ms: 28645
        cost_usd: 0.0051
        measured: 2026-03-01
        git_sha: "8c9e0a1"
        result_file: benchmarks/results/scene-extraction-post-golden-verify.json
        note: "Budget option — cheap but decent"
      - model: "GPT-4.1"
        metrics:
          overall: 0.733
        latency_ms: 24023
        cost_usd: 0.0242
        measured: 2026-03-01
        git_sha: "8c9e0a1"
        result_file: benchmarks/results/scene-extraction-post-golden-verify.json
      - model: "Sonnet 4.5"
        metrics:
          overall: 0.631
        latency_ms: 39700
        cost_usd: 0.0564
        measured: 2026-03-01
        git_sha: "8c9e0a1"
        result_file: benchmarks/results/scene-extraction-post-golden-verify.json
        note: "Down from 0.762 (-0.131) — was rewarded for fabricated scene details"

    attempts: []

  - id: normalization
    name: Normalization
    type: quality
    description: >
      Convert prose narrative or broken Fountain to valid Fountain screenplay
      format. Tests format understanding and faithful content preservation.
      Gemini providers had parse errors during initial run.
    pipeline_stage: normalize
    runner: promptfoo
    command: "cd benchmarks && promptfoo eval -c tasks/normalization.yaml --no-cache -j 3"
    config: benchmarks/tasks/normalization.yaml
    scorer: benchmarks/scorers/normalization_scorer.py
    golden: benchmarks/golden/normalize-signal-golden.json
    test_cases: 2  # prose, broken fountain

    target:
      metric: overall
      value: 0.97
      latency_ms_max: 10000
      cost_usd_max: 0.02

    scores:
      - model: "GPT-4.1"
        metrics:
          overall: 0.961
        latency_ms: 3250
        cost_usd: 0.0044
        measured: 2026-03-02
        git_sha: "8c9e0a1"
        result_file: benchmarks/results/normalization-verify-eval-v2.json
        note: "Best cost-quality-latency balance. Verified: golden scene keywords simplified."
      - model: "Opus 4.6"
        metrics:
          overall: 0.955
        latency_ms: 5465
        cost_usd: 0.0155
        measured: 2026-03-02
        git_sha: "8c9e0a1"
        result_file: benchmarks/results/normalization-verify-eval-v2.json
      - model: "Gemini 2.5 Pro"
        metrics:
          overall: 0.955
        latency_ms: 29716
        cost_usd: 0.0428
        measured: 2026-03-02
        git_sha: "8c9e0a1"
        result_file: benchmarks/results/normalization-verify-eval-v2.json
      - model: "Haiku 4.5"
        metrics:
          overall: 0.954
        latency_ms: 2349
        cost_usd: 0.0030
        measured: 2026-03-02
        git_sha: "8c9e0a1"
        result_file: benchmarks/results/normalization-verify-eval-v2.json
        note: "Fastest + cheapest in top tier. Outstanding value."
      - model: "Sonnet 4.5"
        metrics:
          overall: 0.950
        latency_ms: 5536
        cost_usd: 0.0095
        measured: 2026-03-02
        git_sha: "8c9e0a1"
        result_file: benchmarks/results/normalization-verify-eval-v2.json
      - model: "GPT-4.1 Mini"
        metrics:
          overall: 0.945
        latency_ms: 7902
        cost_usd: 0.0009
        measured: 2026-03-02
        git_sha: "8c9e0a1"
        result_file: benchmarks/results/normalization-verify-eval-v2.json
        note: "Near-SOTA quality at $0.001 — strong budget option"
      - model: "Sonnet 4.6"
        metrics:
          overall: 0.938
        latency_ms: 4491
        cost_usd: 0.0092
        cost_estimated: true
        measured: 2026-03-02
        git_sha: "8c9e0a1"
        result_file: benchmarks/results/normalization-post-golden-verify.json
        note: "Opus still underperforms Sonnet — over-edits content"

    attempts: []

  - id: scene-enrichment
    name: Scene Enrichment
    type: quality
    description: >
      Scene-level metadata enrichment — dramatic beats, tone shifts,
      character presence, location mapping. Tests nuanced scene analysis.
    pipeline_stage: enrich
    runner: promptfoo
    command: "cd benchmarks && promptfoo eval -c tasks/scene-enrichment.yaml --no-cache -j 3"
    config: benchmarks/tasks/scene-enrichment.yaml
    scorer: benchmarks/scorers/scene_enrichment_scorer.py
    golden: benchmarks/golden/enrich-scenes-golden.json
    test_cases: 2  # elevator scene, flashback scene

    target:
      metric: overall
      value: 0.93
      latency_ms_max: 15000
      cost_usd_max: 0.05

    scores:
      - model: "Sonnet 4.6"
        metrics:
          overall: 0.890
        latency_ms: 10662
        cost_usd: 0.0106
        cost_estimated: true
        measured: 2026-02-18
        git_sha: "324ebf4"
        result_file: benchmarks/results/scene-enrichment-2026-02-18.json
      - model: "Opus 4.6"
        metrics:
          overall: 0.812
        latency_ms: 12282
        cost_usd: 0.0194
        measured: 2026-02-18
        git_sha: "324ebf4"
      - model: "GPT-5.2"
        metrics:
          overall: 0.799
        latency_ms: 8327
        cost_usd: 0.0086
        measured: 2026-02-18
        git_sha: "324ebf4"
      - model: "Sonnet 4.5"
        metrics:
          overall: 0.784
        latency_ms: 8637
        cost_usd: 0.0095
        measured: 2026-02-18
        git_sha: "324ebf4"
      - model: "Gemini 3 Flash"
        metrics:
          overall: 0.755
        latency_ms: 8020
        cost_usd: 0.0039
        measured: 2026-02-18
        git_sha: "324ebf4"
        result_file: benchmarks/results/scene-enrichment-gemini-2026-02-18.json
      - model: "Gemini 3 Pro"
        metrics:
          overall: 0.755
        latency_ms: 25317
        cost_usd: 0.0269
        measured: 2026-02-18
        git_sha: "324ebf4"
        result_file: benchmarks/results/scene-enrichment-gemini-2026-02-18.json

    attempts: []

  - id: qa-pass
    name: QA Pass
    type: quality
    description: >
      QA gate — model judges whether an extraction is good or bad.
      Tests calibrated judgment (accept good work, reject bad work).
      Most models perform well; near-ceiling for mid-tier+.
    pipeline_stage: qa
    runner: promptfoo
    command: "cd benchmarks && promptfoo eval -c tasks/qa-pass.yaml --no-cache -j 3"
    config: benchmarks/tasks/qa-pass.yaml
    scorer: benchmarks/scorers/qa_pass_scorer.py
    golden: benchmarks/golden/qa-pass-golden.json
    test_cases: 2  # good extraction, bad extraction

    target:
      metric: overall
      value: 1.0
      latency_ms_max: 10000
      cost_usd_max: 0.02
      note: "QA should be near-perfect — false positives waste pipeline budget"

    scores:
      - model: "GPT-4.1 Mini"
        metrics:
          overall: 1.000
        latency_ms: 4181
        cost_usd: 0.0008
        measured: 2026-03-01
        git_sha: "8c9e0a1"
        result_file: benchmarks/results/qa-pass-post-golden-verify.json
        note: "Production winner — perfect score, cheapest, fast"
      - model: "GPT-5.2"
        metrics:
          overall: 1.000
        latency_ms: 5049
        cost_usd: 0.0068
        measured: 2026-03-01
        git_sha: "8c9e0a1"
        result_file: benchmarks/results/qa-pass-post-golden-verify.json
      - model: "Gemini 2.5 Flash"
        metrics:
          overall: 1.000
        latency_ms: 8521
        cost_usd: 0.0054
        measured: 2026-03-01
        git_sha: "8c9e0a1"
        result_file: benchmarks/results/qa-pass-post-golden-verify.json
      - model: "Gemini 3 Flash"
        metrics:
          overall: 1.000
        latency_ms: 11953
        cost_usd: 0.0064
        measured: 2026-03-01
        git_sha: "8c9e0a1"
        result_file: benchmarks/results/qa-pass-post-golden-verify.json
      - model: "Gemini 3 Pro"
        metrics:
          overall: 1.000
        latency_ms: 46503
        cost_usd: 0.0184
        measured: 2026-03-01
        git_sha: "8c9e0a1"
        result_file: benchmarks/results/qa-pass-post-golden-verify.json
        note: "Up from 0.975 — golden fix corrected phantom issue penalty"
      - model: "Sonnet 4.6"
        metrics:
          overall: 0.995
        latency_ms: 9528
        cost_usd: 0.0116
        cost_estimated: true
        measured: 2026-03-01
        git_sha: "8c9e0a1"
        result_file: benchmarks/results/qa-pass-post-golden-verify.json
      - model: "Opus 4.6"
        metrics:
          overall: 0.995
        latency_ms: 9458
        cost_usd: 0.0176
        measured: 2026-03-01
        git_sha: "8c9e0a1"
        result_file: benchmarks/results/qa-pass-post-golden-verify.json

    attempts: []

  - id: continuity-extraction
    name: Continuity Extraction
    type: quality
    description: >
      Entity state tracking and continuity change detection between scenes.
      Tests temporal reasoning — what changed, what persisted, what's new.
      Newest eval (Story 092). Best-performing overall.
    pipeline_stage: extract
    runner: promptfoo
    command: "cd benchmarks && promptfoo eval -c tasks/continuity-extraction.yaml --no-cache -j 3"
    config: benchmarks/tasks/continuity-extraction.yaml
    scorer: benchmarks/scorers/continuity_extraction_scorer.py
    golden: benchmarks/golden/continuity-extraction-golden.json
    test_cases: 2  # dock day, dock night

    target:
      metric: overall
      value: 0.98
      latency_ms_max: 30000
      cost_usd_max: 0.05

    scores:
      - model: "Sonnet 4.6"
        metrics:
          overall: 0.982
        latency_ms: 31835
        cost_usd: 0.0404
        cost_estimated: true
        measured: 2026-03-01
        git_sha: "8c9e0a1"
        result_file: benchmarks/results/continuity-extraction-post-golden-verify.json
        note: "Up from 0.978 — meets target"
      - model: "GPT-4.1"
        metrics:
          overall: 0.959
        latency_ms: 20022
        cost_usd: 0.0176
        measured: 2026-03-01
        git_sha: "8c9e0a1"
        result_file: benchmarks/results/continuity-extraction-post-golden-verify.json
        note: "New entry — strong mid-tier option"
      - model: "GPT-5.2"
        metrics:
          overall: 0.952
        latency_ms: 15486
        cost_usd: 0.0247
        measured: 2026-03-01
        git_sha: "8c9e0a1"
        result_file: benchmarks/results/continuity-extraction-post-golden-verify.json
      - model: "Opus 4.6"
        metrics:
          overall: 0.949
        latency_ms: 23120
        cost_usd: 0.0554
        measured: 2026-03-01
        git_sha: "8c9e0a1"
        result_file: benchmarks/results/continuity-extraction-post-golden-verify.json
      - model: "Sonnet 4.5"
        metrics:
          overall: 0.932
        latency_ms: 16545
        cost_usd: 0.0254
        measured: 2026-03-01
        git_sha: "8c9e0a1"
        result_file: benchmarks/results/continuity-extraction-post-golden-verify.json
      - model: "Gemini 2.5 Flash"
        metrics:
          overall: 0.922
        latency_ms: 12832
        cost_usd: 0.0083
        measured: 2026-03-01
        git_sha: "8c9e0a1"
        result_file: benchmarks/results/continuity-extraction-post-golden-verify.json
      - model: "Haiku 4.5"
        metrics:
          overall: 0.885
        latency_ms: 8842
        cost_usd: 0.0095
        measured: 2026-03-01
        git_sha: "8c9e0a1"
        result_file: benchmarks/results/continuity-extraction-post-golden-verify.json
        note: "Down from 0.948 — was rewarded for incomplete patterns"

    attempts: []

  # ── Compromise Evals ──────────────────────────────────────────────────────
  # Populated by /bootstrap-compromise-evals from docs/spec.md.
  # Each tests whether a spec compromise can be eliminated.

  - id: compromise-C2-qa-validation
    name: "Compromise C2: Dedicated QA Passes"
    type: compromise
    compromise_id: "C2"
    description: >
      Can we eliminate dedicated QA validation passes? Gate: 10 diverse
      extraction tasks to SOTA, all pass structural + semantic on first
      attempt without QA retry loop.
    detection_mechanism: >
      Run 10 diverse extraction tasks through SOTA model. All must pass
      both structural (Python scorer) and semantic (LLM rubric) quality
      gates on the first attempt, without any QA-triggered retry.
    ideal_link: "docs/ideal.md"
    spec_link: "docs/spec.md#2.8"
    runner: custom  # needs a dedicated test harness
    command: null  # not yet implemented
    scores: []
    attempts: []

  - id: compromise-C3-tiered-models
    name: "Compromise C3: Tiered Model Strategy"
    type: compromise
    compromise_id: "C3"
    description: >
      Can we use a single model for everything? Gate: one model achieves
      top-tier quality on ALL eval tasks at acceptable latency and cost.
      This is the meta-eval — it checks if any single model tops every
      individual eval's target.
    detection_mechanism: >
      Single model achieves top-tier quality on all eval tasks:
      character, location, prop, scene extraction, normalization,
      enrichment, QA, config detection, relationship discovery,
      and continuity extraction. Check by scanning registry scores.
    ideal_link: "docs/ideal.md"
    spec_link: "docs/spec.md#2.9"
    runner: registry-scan  # can be computed from existing eval scores
    command: null  # scan registry.yaml programmatically
    scores: []
    attempts: []

  - id: compromise-C4-two-tier-scenes
    name: "Compromise C4: Two-Tier Scene Architecture"
    type: compromise
    compromise_id: "C4"
    description: >
      Can we do full scene analysis in one pass? Gate: SOTA model returns
      complete scene analysis (boundaries + enrichment + entity tracking)
      with high quality AND under 5 seconds.
    detection_mechanism: >
      Full screenplay to SOTA requesting complete scene analysis —
      high quality (≥0.90 on scene extraction + enrichment combined)
      AND returns in <5 seconds per scene.
    ideal_link: "docs/ideal.md"
    spec_link: "docs/spec.md#5.1"
    runner: custom
    command: null
    scores: []
    attempts: []

  - id: compromise-C5-role-modality
    name: "Compromise C5: Role Capability Gating (Modality)"
    type: compromise
    compromise_id: "C5"
    description: >
      Can roles process all modalities? Gate: SOTA model reliably handles
      text + image + video + audio in a single call.
    detection_mechanism: >
      SOTA model reliably processes text + image + video + audio in a
      single call with coherent cross-modal reasoning.
    ideal_link: "docs/ideal.md"
    spec_link: "docs/spec.md#8.2"
    runner: custom
    command: null
    scores: []
    attempts: []

  - id: compromise-C7-working-memory
    name: "Compromise C7: Working Memory Distinction"
    type: compromise
    compromise_id: "C7"
    description: >
      Can we eliminate the working memory abstraction? Gate: context windows
      exceed 10M tokens at negligible cost, OR native persistent cross-session
      memory becomes available.
    detection_mechanism: >
      Context windows exceed 10M tokens at negligible cost, OR
      native persistent cross-session memory in SOTA models.
    ideal_link: "docs/ideal.md"
    spec_link: "docs/spec.md#19.2"
    runner: capability-check  # check model specs, not run code
    command: null
    scores: []
    attempts: []
