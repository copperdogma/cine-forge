# ADR-002: Goal-Oriented Project Navigation

**Status:** DECIDED â€” Four-layer hybrid architecture (Pipeline Bar + AI Navigator + DAG View + Persona Workspaces)

## Context

CineForge has a large pipeline: script ingestion â†’ normalization â†’ scene extraction â†’ entity bibles â†’ creative direction (editorial/visual/sound) â†’ direction convergence â†’ shot planning â†’ storyboards â†’ render â†’ final output. Each stage produces artifacts that feed downstream stages.

**The problem:** different users want different things from this pipeline, and the app has no way to know what they want or guide them through it.

- A **screenwriter** uploads a script and wants analysis: characters, scenes, relationships. They don't care about shot planning or rendering.
- A **director** wants full pre-production: creative direction from all roles, converged direction, shot lists, storyboards.
- A **producer** wants to drop in a script, press a button, and get a finished film with minimal input.
- A **curious explorer** just wants to see what the tool can do â€” they'll click around, try things, change their mind.

Today the app surfaces capabilities as hidden buttons on individual pages (e.g., "Get Editorial Direction" on the scene Direction tab). There's no persistent view of what's been done, what's available, or what to do next. The operating modes from Story 019 (autonomous/checkpoint/advisory) control *how much the AI does on its own* but not *what the user is trying to accomplish*.

**The "everything is wrong" problem:** A user clicks "generate video" for Scene 3. The output looks terrible â€” wrong mood, wrong style, wrong character appearance, wrong sound. Why? Because they skipped visual direction, sound design, character design studies â€” all the upstream stages that would have informed the render. The system generated the video using defaults and guesses. The user has no idea what went wrong or how to fix it. The AI assistant *also* doesn't know, because there's no structured representation of what's been done vs. what's missing.

**Goal evolution:** Users change their minds. Someone starts with "just analyze my script," sees the direction annotations, gets excited, and decides they want storyboards. The system needs to show them what exists, what's missing for their new goal, and how to get there â€” without starting over.

**The inbox isn't this:** The inbox (Story 069) is a log of things that happened. It doesn't track progress toward a goal, show what's missing, or guide next steps.

### What prompted this

During triage of stories 021-024 (creative direction roles), we realized: building more direction types without a way to surface them is just adding more hidden buttons. The app needs a system that makes the full pipeline visible, goal-aware, and navigable â€” before we build more stages into it.

## The Ideal (No Technology Constraints)

An AI-readable **capability graph** where:

- Every user-facing capability (script analysis, character bibles, visual direction, shot planning, video render, etc.) is a node
- Edges represent dependencies (video render depends on shot plan, which depends on converged direction, which depends on visual + sound + editorial direction)
- Each node tracks status: not started / generated / reviewed / user-customized / user-skipped
- The AI assistant can traverse this graph to:
  - Answer "what should I do next?" based on the user's stated goal
  - Diagnose "why does this output look bad?" by identifying missing or default upstream nodes
  - Suggest "you could improve this by..." with specific upstream actions
- Users can:
  - State goals ("I want storyboards for Act 1") and the system derives required steps
  - Add/change goals at any time, and the system adjusts what it surfaces
  - See a persistent progress view: what's done, what's next, what's available, what they've skipped
  - Click "generate video" even when upstream is incomplete â€” the system doesn't block, but explains what's missing and offers to help fix it
  - Jump into any node directly and the system shows prerequisites
- Goals have prerequisites that can be fulfilled by the user (import existing assets) OR generated by the system (autonomously or interactively)
- The operating modes (autonomous/checkpoint/advisory from Story 019) become execution strategies for goal steps, not user-facing choices
- The capability graph is the single source of truth for both the AI and the UI

## Options

### Option A: AI-Readable Capability Graph (State Machine)

Build a formal graph data structure that represents the full pipeline at a user-facing abstraction level (above individual artifacts). Each node has: capability name, input requirements, output artifacts, status, and dependencies. The AI reads this graph to reason about project state. The UI renders it as a progress view.

**Pros:**
- AI can reason precisely about missing prerequisites
- Progress visualization falls out naturally
- Goal derivation is a graph traversal problem
- New pipeline stages plug in by adding nodes

**Cons:**
- Requires maintaining graph in sync with actual pipeline capabilities
- Graph may become complex as capabilities grow
- Upfront design effort

### Option B: AI-Only Guidance (No Formal Graph)

Enhance the chat AI's system prompt with full pipeline knowledge. The AI reads artifact store state (what exists, what's stale) and reasons about what's missing. No formal graph data structure â€” the AI IS the graph.

**Pros:**
- No new data model to maintain
- AI reasoning is flexible, handles edge cases naturally
- Simpler implementation

**Cons:**
- AI guidance is conversational only â€” no persistent visual progress
- AI may give inconsistent advice across sessions
- Can't support goal-aware UI (checklists, progress bars) without a data model
- Hard to test or validate AI's pipeline reasoning

### Option C: Recipe-Based Goal Templates

Define named goal templates (e.g., "Script Analysis," "Full Pre-Production," "Quick Render") that map to specific recipe configurations. Each template is a subset of the full pipeline with a predefined sequence. User picks a template at project creation (or later), and the system shows a checklist for that template.

**Pros:**
- Simple UX â€” pick a template, follow the checklist
- Easy to implement â€” templates are just recipe + UI config
- Predictable, testable behavior

**Cons:**
- Rigid â€” templates can't handle "I changed my mind" or "I want to skip visual direction"
- Combinatorial explosion as capabilities grow
- Doesn't solve the diagnosis problem ("why is my render bad?")
- Users who don't fit a template fall through the cracks

### Option D: Hybrid â€” Graph Backend + Template Shortcuts

Build the formal capability graph (Option A) but provide named templates (Option C) as entry points. Templates pre-select a goal and path through the graph. Users who outgrow templates can see and navigate the full graph. AI reads the graph regardless.

**Pros:**
- Simple onboarding (pick a template) with power-user depth (see the graph)
- AI has structured data for diagnosis
- Templates are just pre-configured graph traversals, not a separate system
- Goal evolution works â€” switch templates or go custom

**Cons:**
- Most complex to build
- Need to get both the graph design AND the template UX right

## Research Needed

- [ ] How do other creative production tools (Runway, Kling, Pika, Adobe Premiere, DaVinci Resolve, Final Cut) handle multi-stage workflows with optional steps? What UX patterns do they use for progress tracking?
- [ ] What are the best patterns for dependency-aware progress UIs in complex workflows? (CI/CD pipelines like GitHub Actions, game tech trees, project management tools like Linear/Notion)
- [ ] How should an AI assistant reason about a dependency graph to provide useful guidance? What graph representations are most LLM-friendly?
- [ ] What onboarding patterns work best for tools with multiple user personas and goals? (Wizard flows, progressive disclosure, template selection, none-of-the-above)
- [ ] How do existing AI-first creative tools handle the "everything is wrong because you skipped upstream" problem? Do they block, warn, or let you fail and diagnose?

## Decision

**Option D: Hybrid â€” Pipeline Bar + AI Navigator + On-Demand DAG + Persona Workspaces**

All four research reports (OpenAI, Google, Claude Opus, Grok) converged on this architecture independently. Full synthesis: `docs/research/adr-002-goal-oriented-navigation/final-synthesis.md`.

### The Four-Layer Architecture (in build order)

**Layer 1 â€” Persistent Pipeline Bar** *(ship first)*
Horizontal bar (DaVinci Resolve-style) showing all pipeline phases as clickable tabs with three-state status: completed (green) / available (highlighted) / blocked (dimmed). Each tab shows a badge ("3/5 scenes extracted"). Adapts to persona: Screenwriter sees Script/Entities/Analysis prominent; Director sees Visual Direction/Shot Plan/Storyboard prominent. All phases accessible to all users regardless of persona.

**Layer 2 â€” Conversational AI Navigator** *(ship simultaneously with Layer 1)*
The existing chat sidebar becomes the primary navigation intelligence. The AI reads the project's dependency graph and:
- At **workflow boundaries** (stage completion): surfaces what's unlocked, recommends 1-2 next steps
- At **downstream action triggers**: runs a preflight check â€” ðŸŸ¢ proceed / ðŸŸ¡ warn + offer fix / ðŸ”´ soft block
- At **user request**: plan-then-execute â€” presents structured plan before running expensive operations
- AI graph context is scoped to the local subgraph (2-3 hops from current focus) as natural language, with full graph available as JSON for tool calls
- Three verbosity levels: Novice / Standard / Expert

**Layer 3 â€” On-Demand DAG View** *(v2)*
Full pipeline dependency graph, accessible via keyboard shortcut or "Pipeline Map" button. Left-to-right DAG with phase columns, hover-to-trace, goal-backward navigation (click locked node â†’ see prerequisite path â†’ "Start this path"), semantic zoom (phase clusters at overview, nodes at detail).

**Layer 4 â€” Persona-Adaptive Workspaces** *(evolve over time)*
Screenwriter / Director / Producer / Explorer personas with purpose-built default views on shared project data. Three AI autonomy levels per feature: Automatic / Assisted / Manual.

### The Tiered Response System (for "skipped upstream")

When a user triggers a downstream action:
- ðŸŸ¢ All upstream complete â†’ proceed silently
- ðŸŸ¡ Some upstream missing but output still meaningful â†’ warning card with [Fix First] and [Proceed with Placeholder]
- ðŸ”´ Critical upstream missing, output meaningless â†’ soft block with one-click fix path
- For expensive operations (render/export): always show pre-flight summary with estimated quality

Never auto-generate upstream content to fill gaps without clearly marking every placeholder. Silent fallback is the worst possible pattern.

### Onboarding

One question at project creation: "I'm a..." [Screenwriter / Director / Producer / Just Exploring]. Skippable, defaults to Explorer. Selection configures default workspace, pipeline bar emphasis, AI verbosity, and template suggestions. Goal changes via "Switch Role" in project settings â€” no artifacts lost.

### What Operating Modes (Story 019) Become

Autonomous/Checkpoint/Advisory become execution strategies for how goal steps run, not user-facing primary choices. Surfaced as per-feature autonomy level (Automatic / Assisted / Manual) rather than a global project switch.

### Build Order

1. Define the pipeline graph schema (structured JSON: nodes, edges, status, staleness)
2. Implement pipeline bar wired to graph schema
3. Build AI graph-reading tools (local subgraph query + natural-language rendering)
4. Implement tiered response interceptor for all generation actions
5. Pre-flight check UI for expensive operations (render/export)
6. Staleness propagation with visual indicators
7. On-demand DAG view (Phase 2)
8. Persona onboarding + adaptive workspaces (Phase 3)

## Legacy Context

None â€” greenfield concept. No prior CineForge version had goal-oriented navigation.

Closest existing systems:
- **Story 019 (Done):** Operating modes (autonomous/checkpoint/advisory). These become implementation details of how goal steps execute, not user-facing choices.
- **Story 011e (Done):** UX Golden Path â€” designed "suggested next action" pattern and progressive materialization. The principle: "if the user has to think about what to do next, the UI has failed." But current implementation is a single chat suggestion, not a structured system.
- **Story 011f (Done):** Conversational AI Chat â€” AI reads project state and gives contextual recommendations. Works for "what should I do next?" but conversational, not visual, and not goal-aware.
- **Inbox item:** Cam's note capturing the exact problem â€” quoted in full in the Context section above.

## Dependencies

- Affects how stories 021-024 (creative direction roles) are surfaced to users
- Affects Story 024 (direction convergence) â€” may need only 2-3 direction types depending on user goal
- Affects Story 025 (shot planning) â€” entry point depends on what upstream exists
- May inform Story 056 (entity design studies) â€” another capability node in the graph
- Depends on existing artifact store (Story 002) for status data
- Depends on recipe system for pipeline stage definitions
