---
type: research-report
topic: adr-002-goal-oriented-navigation
canonical-model-name: o4-mini-deep-research
research-mode: deep
collected: '2026-02-25T23:25:16.600179+00:00'
---

# 1. Multi-Stage Creative Tool Workflows

**Recommended:** We saw that modern **AI video tools** increasingly use explicit *pipeline or node-based workflows*. For example, Runway’s new *Workflows* feature lets users chain models and modalities in a node graph ([runwayml.com](https://runwayml.com/workflows#:~:text=Combine%20multiple%20models%2C%20modalities%20and,tasks)), automatically feeding outputs of one step into the next (text → image → video, etc.). Tools like LTX Studio/StoryboardHero will auto-extract scenes or storyboards from scripts. **Best practice:** provide a **visual pipeline overview** (e.g. a node graph or flowchart) that shows all major stages (script analysis, scene planning, editorial direction, visual direction, shot planning, render, etc.), and updates as each completes. Show status icons (green for done, grey for pending) on each stage – Houdini’s PDG pipeline does this with color-coded work-item dots and progress rings on each node ([www.sidefx.com](https://www.sidefx.com/docs/houdini/tops/ui.html#:~:text=Dot%20%20,work%20item%20generated%20a%20warning)) ([www.sidefx.com](https://www.sidefx.com/docs/houdini/tops/ui.html#:~:text=To%20the%20left%20of%20each,not%20included%20in%20the%20progress)). For example, a “shot planning” node might display a spinning ring while AI is generating shots, and a green check when finished. Advanced users (e.g. VFX artists) can click nodes to inspect outputs, while novices see a simplified linear/storyboard view of progress. 

**Anti-pattern:** Scattering independent action buttons (e.g. “Get Editorial Direction” on a scene page) with no context. In many early AI-video tools (Pika, Luma) features felt siloed: users had to manually copy an image into video-gen prompts, etc. This leads to confusion and poor results if steps are skipped. We should avoid giving full control without context. Instead, just as **compilers** error out if a source file is missing, our pipeline UI should either *disable* a downstream step until upstream work exists, or at least warn the user (“VisualStyle incomplete – render may be poor”). If we let a user jump ahead (e.g. doing *video render* before *visual direction*), show a prominent tooltip: *“Warning: Visual style data is missing; results will be unstyled.”* and indicate the missing dependency.

**Examples:** Runway Workflows (node/pipeline mode) is one model to mimic ([runwayml.com](https://runwayml.com/workflows#:~:text=Combine%20multiple%20models%2C%20modalities%20and,tasks)). Cinematic VFX tools (Nuke, Houdini) use node graphs: e.g. Houdini’s TOP network shows a node for each “task” with dots and colored status rings ([www.sidefx.com](https://www.sidefx.com/docs/houdini/tops/ui.html#:~:text=Dot%20%20,work%20item%20generated%20a%20warning)). Game engines like Unreal have a **Reference Viewer** that shows asset dependency graphs (e.g. a Material node pointing to a Texture node) ([dev.epicgames.com](https://dev.epicgames.com/documentation/en-us/unreal-engine/reference-viewer-in-unreal-engine#:~:text=The%20Reference%20Viewer%20displays%20a,by%20one%20or%20more%20Assets)). For music, DAWs like Ableton Live guide users through *clip creation, arrangement, mixing* via tracks/timelines – a useful metaphor. We’d show **tracks or phases** (script → storyboard → animatic → audio → mix → final) as lanes.


# 2. Dependency-Aware Progress UI

We need a UI that visualizes a DAG of steps (not just a strict linear wizard). Good precedents:

- **CI/CD graphs:** GitHub Actions displays a real-time DAG of jobs with status icons and connecting arrows ([docs.github.com](https://docs.github.com/actions/managing-workflow-runs/using-the-visualization-graph#:~:text=5,Lines%20between%20jobs%20indicate%20dependencies)). Parallel branches and dependencies are explicit: you immediately see which jobs are waiting, running, or passed. **Pattern to copy:** use a similar graph or flowchart. Show completed nodes in green, running in orange, failed in red. Lines/arrows show “needs” relationships. (GitHub even lets you click a job to see logs ([docs.github.com](https://docs.github.com/actions/managing-workflow-runs/using-the-visualization-graph#:~:text=5,Lines%20between%20jobs%20indicate%20dependencies)).) Jenkins’ Pipeline Stage View and the GitHub Actions “visualization graph” are good examples of showing progress in a DAG, highlighting which prerequisites remain.  

- **Game “tech trees”:** Strategy games (Civ, Factorio) show a tree of unlocks where prerequisites are evident. On these trees, *available* upgrades are highlighted when their prerequisites are met, and *locked* options are greyed out or marked. Key takeaways: always display **both done and locked future stages** in one view. E.g. gray out or padlock “Scene Storyboard” until the script analysis is done, then highlight it as “unlocked.” Empower the user by letting them click forward to see tooltip info (“Requires: Complete Editorial Direction”). Avoid hiding options – like Civilization, we show the entire tree but make prerequisites clear.

- **Project boards (Linear/Asana/Notion):** Modern tools let you link tasks with “blocked by” dependencies. For example, Linear’s **timeline view** draws arrows from a finished project to the next that depends on it ([linear.app](https://linear.app/docs/project-dependencies#:~:text=When%20a%20dependency%20relationship%20has,fields)). A dependency line turns **red if violated** (e.g. a dependent item is scheduled before its predecessor) ([linear.app](https://linear.app/docs/project-dependencies#:~:text=Dependency%20line%20color%E2%81%A0)). We should similarly color links: blue (normal) vs. red (broken). Asana also displays chains on its Timeline (with options to prevent completing a blocked task) – a “blocked by” tag or column is visible on each item. Monday.com has a **dependency column** which, when set to “Strict,” will automatically shift dependent item dates, visibly dragging the chain in the Gantt view ([support.monday.com](https://support.monday.com/hc/en-us/articles/360007402599-Dependencies-on-monday-com#:~:text=only%20this%2C%20but%20when%20dragging,%F0%9F%99%8C)). The pattern: on a timeline/Gantt, drawing connectors between tasks clearly shows constraints, and dragging one will snap its dependents.

- **Wizards with optional steps:** For example, Material Design “steppers” mark optional steps as “(optional)” up front. On a multi-tab setup form, an optional step has a “SKIP” or Next button always enabled (ensuring skipping still counts as ‘complete’). One UX answer notes: *“If a step is optional, there must be affordance…Add an ‘Optional’ label in the step indicator or a note at the top.”* ([ux.stackexchange.com](https://ux.stackexchange.com/questions/126493/stepper-wizard-optional-step-skip-and-next-button#:~:text=If%20a%20step%20is%20optional,of%20options%20you%20can%20do)). We’ll do the same. Show optional stages as such, so users know they can jump ahead. In Cinemeforge’s chat interface, we might surface an optional step by saying “(optional)” next to it, and allow the user to skip it without penalty.

**Summary:** The UI should make it **clear what’s done and what’s possible next** in a DAG. For required steps, disable or warn on Next until dependencies exist; for optional steps, clearly label them and keep “Next” active. Using progress-check visuals (checkboxes or rings) combined with a dependency graph (arrows or indented lists) will scale up beyond a trivial linear stepper. We can display 5–30 nodes by grouping related tasks (e.g. under “Pre-production” vs “Production”) and allowing zoom or filtering, much like CI tools let you collapse branches. This combined approach – checklists plus dependency graph – makes the pipeline navigable but not overwhelming.


# 3. AI Assistant Reasoning & Planning

**Representation:** LLMs understand structured JSON or bullet lists better than arbitrary ad-hoc text. It’s ideal to feed the assistant a clear representation of the current project’s **capability graph**. For example, an adjacency-list JSON of tasks, or even a Mermaid-style graph in markdown, could be provided as context. The Claude Code “structured-plan-mode” suggests storing tasks as separate Markdown files with metadata (status, blocked-by) ([nikiforovall.blog](https://nikiforovall.blog/claude-code-rules/component-reference/skills/structured-plan-mode/#:~:text=4.%20,md%20Progress%20Summary)) ([nikiforovall.blog](https://nikiforovall.blog/claude-code-rules/component-reference/skills/structured-plan-mode/#:~:text=2,Status%3A%20%F0%9F%9F%A2%20In%20Progress)). We should similarly teach the AI about our pipeline: e.g. a JSON like `{ "tasks": { "ScriptAnalysis": {}, "Storyboarding": {"dependsOn": ["ScriptAnalysis"]}, ... }}`. This makes it easy for the LLM to check “Can I do X if Y isn’t done?” 

**Planning pattern:** Coding assistants use *“plan mode”* for complex tasks. GitHub Copilot’s Extended Plan Mode breaks a feature into phases (context, planning, implementation, evaluation) ([nikiforovall.blog](https://nikiforovall.blog/github-copilot-rules/guides/extended-plan-mode#:~:text=Stage%201%3A%20Context%20Priming%E2%80%8B)). Claude Code’s structured-plan-mode explicitly breaks work into phases and even task files ([nikiforovall.blog](https://nikiforovall.blog/claude-code-rules/component-reference/skills/structured-plan-mode/#:~:text=Skill%20Specification%E2%80%8B)) ([nikiforovall.blog](https://nikiforovall.blog/claude-code-rules/component-reference/skills/structured-plan-mode/#:~:text=4.%20,md%20Progress%20Summary)). We can adopt this: prompt the assistant to first outline all required steps (e.g. “You’re Director: list steps from script to final video”), then execute them one by one. Using conversation memory, the AI can mark tasks “In Progress” or “Done.” It can keep track of a “Progress Summary” list, updating it as each stage completes ([nikiforovall.blog](https://nikiforovall.blog/claude-code-rules/component-reference/skills/structured-plan-mode/#:~:text=2,Status%3A%20%F0%9F%9F%A2%20In%20Progress)). 

**Explaining bad output:** When results are poor, the AI should *diagnose missing inputs*. For example, it might say: “This scene’s storyboard is low-resolution because no visual style or camera instructions were provided (scene editors use ‘VisualDirection’ data).” It should point to the earliest unmet dependency (“Your shot plan lacks EditorialDirection, so I did my best but it’s unfocused.”) without overwhelming detail. The key is framing it in user terms (“We didn’t do X, so output lacks Y”). The assistant should **proactively** offer help if it detects a missed prerequisite (e.g. “It seems you haven’t run visual style generation. Would you like help?”), but only when it’s obvious – so as not to annoy. A safe rule: if a generated artifact is much lower quality than usual and a dependency scan shows upstream data is missing or stale, the AI should flag it and suggest upstream steps. 

**Agent frameworks:** As in Microsoft’s AutoGen or similar multi-agent systems, we could assign roles (Planner, Developer, QA) that coordinate on the pipeline. For instance, a “Story Editor” agent might ensure script consistency before “Director” agent plans shots—this mirrors multi-agent tools that improve “complicated multi-step tasks” by specialization ([techcommunity.microsoft.com](https://techcommunity.microsoft.com/blog/azure-ai-foundry-blog/build-your-dream-team-with-autogen/4157961#:~:text=The%20idea%20of%20using%20LLMs,can%20execute%20various%20complicated%20tasks)) ([nikiforovall.blog](https://nikiforovall.blog/github-copilot-rules/guides/extended-plan-mode#:~:text=Stage%201%3A%20Context%20Priming%E2%80%8B)). In practice, we’d let the assistant @-mention the relevant role (“@Cherry (Story Editor): I think we need to refine the plot before proceeding.”). This uses the pipeline graph as a shared knowledge base.  

# 4. Goal-Oriented Onboarding & Personas

**Persona entry:** The tool should gently gauge or let the user pick their goal. Some apps prompt user types (e.g. **“What would you like to do first?”**) or show templates. Canva doesn’t directly ask “Are you a student or a marketer?”, but it does present template categories (social media, presentations, etc.) that implicitly guide goals. Figma, by contrast, often asks about project goals (design file, wireframe, prototype). Notion’s template gallery serves a similar role: users pick a template (note, project plan, wiki) which reveals how Notion can meet that need. 

**Goal selection trade-offs:** Explicitly asking “What do you want to achieve?” can disambiguate early, but we must avoid long forms. A good compromise is a *one-click profile/question step* (e.g. “I’m a Screenwriter / Director / Producer”). If skipped, the system should infer from behavior (e.g. if user only generates character profiles, assume “script analysis” goal). But we should allow changing goals at any time – the UI can have a “Project Goal” dropdown. Many tools let goals evolve: Notion, for example, lets you delete or add templates later, and still all content is in one workspace. If a user initially “just wanted script analysis” but then says “Actually, make me storyboards too,” the tool should simply extend the pipeline and fill in missing artifacts.  

**Templates vs. freeform:** Templates help guide beginners (e.g. “Fantasy short film outline”, “Corporate explainer storyboard”) but advanced users often want freeform. We can combine both: on start, show a few example “project templates” (analysis-only, storyboard-only, full pipeline). The user may choose one or skip to a blank slate. For instance, like Linear’s new “Project dependencies” tutorial, we could have sample workflows: “Start here if you only need script analysis” vs. “Director’s full pre-production”. This guides new users, yet everyone can still jump around: e.g. a director persona might skip any “wizards” and dive straight into shot planning because they’re experienced.

**Handling uncertainty and changes:** If a user isn’t sure what they want, the UI can show the entire pipeline graph faintly (“You can also do these later”). Perhaps a help message: *“Not sure yet? Explore scene breakdown or character dialogs – you can always run the rest later.”* This balances guidance with flexibility. The system should never strictly *force* a path. Instead it should surface suggestions (like a “Next Steps” panel) based on current progress. For example, after script ingestion, it might pop up: *“Story Editor @Riley suggests you proceed to Scene Extraction or Character Profiles.”* 

# 5. Handling “Skipped Upstream” Errors

We examined how tools behave when prerequisites are missing:

- **Render Engines:** If you try to render in 3D with missing textures or geometry, engines often render with placeholders (black or pink textures) and log warnings. For example, Unreal shows pink materials for missing textures but still lets you preview. Maya/Blender may render a blank or error out on a missing file. Generally they **warn heavily** (“Missing texture” msgs) but do *not* completely block a render, so users can still diagnose. We should emulate this: allow the downstream step to run, but annotate the output and show a log/error pop-up stating what was missing.

- **Compilers/Build Systems:** Builds unambiguously **fail fast** on unmet dependencies (missing import, unresolved modules) and display an error message. This usually leads to better outcomes because the developer immediately finds and fixes the cause. We should similarly treat core data-prep failures as *blocking errors* (colored red) and halt that branch. However, as in many build systems, our assistant can also parse the “error” and suggest the fix (“You need to run Visual Direction first.”). That’s better UX than silent failure.

- **AI Image Generators:** When outputs are poor due to an underspecified prompt, most tools (Midjourney, DALL-E) simply encourage iteration. There’s no explicit “error” – the user must experiment. This is user-driven learning. In our tool, we can do better: if a prompt lacks detail, the AI assistant can prompt for clarification (“Shall we refine the style or setting?”) or auto-suggest a richer prompt. But we should not forcibly stop the process; rather, treat it like an advisory hint.

- **3D Printing Slicers:** Slicers like Cura or Prusa will often let you slice imperfect models but report warnings (non-manifold edges, overhangs). Users are given a preview and optional fixes (auto-repair). They usually **warn but allow continuing** (with potentially poor prints). In our pipeline, if a user “slices” a story into scenes with no direction and output is incoherent, we can similarly show a preview storyboard with a big “Warning: missing creative direction” and perhaps a checkbox “Continue anyway”.

**Summary:** In general, the best outcome comes when upstream errors are made obvious, but not necessarily hard-blocked unless irrecoverable. We should *warn early* (“Missing X”) and highlight or annotate the faulty output, but often still show what was produced. Then either prevent progression (like a red error) until fixed, or tag the stage as “Blocked” ([nikiforovall.blog](https://nikiforovall.blog/claude-code-rules/component-reference/skills/structured-plan-mode/#:~:text=2.%20%F0%9F%9F%A2%20,move)) and let the user manually resume it later. This mimics tactics from compilers/builds (fail fast with clear errors) and production tools (placeholder outputs plus warnings). The balance is to make mistakes **learnable**: show the flawed result plus a natural-language explanation of the missing dependency, enabling the user to fix upstream without feeling lost.

 

**Synthesis:** Across all these cases, the coherent approach is to visualize the pipeline as a graph, track each node’s status, and let the AI assistant use that structure. The UI could show a **DAG summary panel** (like a flowchart) alongside a checklist of completed steps. The assistant can say things like “We have completed script analysis and scene breakdown. Next recommended step: @VisualArchitect to define the look of Scene 3.” If a user skips to rendering prematurely, the assistant will reference the DAG (“Note: Scene 3 has no defined style or camera. Output may lack polish.”) and either temporarily block or grey out the render node until upstream work is added. This way, by borrowing patterns from CI graphs, tech trees, node networks, and dependency managers, we turn the pipeline into an explorable, goal-driven system that guides users smoothly from goals to completed tasks.  

**References:** We draw on fields from AI video (Runway Workflows ([runwayml.com](https://runwayml.com/workflows#:~:text=Combine%20multiple%20models%2C%20modalities%20and,tasks)), ReelMind’s multi-model pipeline ([reelmind.ai](https://reelmind.ai/blog/lumos-luma-illuminating-your-ai-video-workflow#:~:text=innovation%20and%20efficiency%20in%20the,ecosystem%20for%20AI%20video%20artistry))) to VFX (Houdini’s task-graph UI ([www.sidefx.com](https://www.sidefx.com/docs/houdini/tops/ui.html#:~:text=Dot%20%20,work%20item%20generated%20a%20warning))) to project tools (GitHub Actions visual graph ([docs.github.com](https://docs.github.com/actions/managing-workflow-runs/using-the-visualization-graph#:~:text=5,Lines%20between%20jobs%20indicate%20dependencies)), Linear’s blocked–blocking timeline ([linear.app](https://linear.app/docs/project-dependencies#:~:text=When%20a%20dependency%20relationship%20has,fields)), Monday.com’s dependency settings ([support.monday.com](https://support.monday.com/hc/en-us/articles/360007402599-Dependencies-on-monday-com#:~:text=only%20this%2C%20but%20when%20dragging,%F0%9F%99%8C)), etc.). User-facing guides on wizards and onboarding also inform our approach ([ux.stackexchange.com](https://ux.stackexchange.com/questions/126493/stepper-wizard-optional-step-skip-and-next-button#:~:text=If%20a%20step%20is%20optional,of%20options%20you%20can%20do)) ([linear.app](https://linear.app/docs/project-dependencies#:~:text=Dependency%20line%20color%E2%81%A0)). These combined insights show how to display *“what’s done, what’s next, and why”* in a flexible, DAG-structured creative pipeline.

## Sources

- [Runway Workflows | Runway AI](https://runwayml.com/workflows#:~:text=Combine%20multiple%20models%2C%20modalities%20and,tasks)
- [PDG node network interface](https://www.sidefx.com/docs/houdini/tops/ui.html#:~:text=Dot%20%20,work%20item%20generated%20a%20warning)
- [PDG node network interface](https://www.sidefx.com/docs/houdini/tops/ui.html#:~:text=To%20the%20left%20of%20each,not%20included%20in%20the%20progress)
- [Runway Workflows | Runway AI](https://runwayml.com/workflows#:~:text=Combine%20multiple%20models%2C%20modalities%20and,tasks)
- [PDG node network interface](https://www.sidefx.com/docs/houdini/tops/ui.html#:~:text=Dot%20%20,work%20item%20generated%20a%20warning)
- [Reference Viewer in Unreal Engine | Unreal Engine 5.7 Documentation | Epic Developer Community](https://dev.epicgames.com/documentation/en-us/unreal-engine/reference-viewer-in-unreal-engine#:~:text=The%20Reference%20Viewer%20displays%20a,by%20one%20or%20more%20Assets)
- [Using the visualization graph - GitHub Docs](https://docs.github.com/actions/managing-workflow-runs/using-the-visualization-graph#:~:text=5,Lines%20between%20jobs%20indicate%20dependencies)
- [Using the visualization graph - GitHub Docs](https://docs.github.com/actions/managing-workflow-runs/using-the-visualization-graph#:~:text=5,Lines%20between%20jobs%20indicate%20dependencies)
- [Project dependencies – Linear Docs](https://linear.app/docs/project-dependencies#:~:text=When%20a%20dependency%20relationship%20has,fields)
- [Project dependencies – Linear Docs](https://linear.app/docs/project-dependencies#:~:text=Dependency%20line%20color%E2%81%A0)
- [Dependencies on monday.com – Support](https://support.monday.com/hc/en-us/articles/360007402599-Dependencies-on-monday-com#:~:text=only%20this%2C%20but%20when%20dragging,%F0%9F%99%8C)
- [material design - stepper wizard, optional step, skip and next button? - User Experience Stack Exchange](https://ux.stackexchange.com/questions/126493/stepper-wizard-optional-step-skip-and-next-button#:~:text=If%20a%20step%20is%20optional,of%20options%20you%20can%20do)
- [structured-plan-mode | Claude Code Handbook](https://nikiforovall.blog/claude-code-rules/component-reference/skills/structured-plan-mode/#:~:text=4.%20,md%20Progress%20Summary)
- [structured-plan-mode | Claude Code Handbook](https://nikiforovall.blog/claude-code-rules/component-reference/skills/structured-plan-mode/#:~:text=2,Status%3A%20%F0%9F%9F%A2%20In%20Progress)
- [Extended Plan Mode | GitHub Copilot Handbook](https://nikiforovall.blog/github-copilot-rules/guides/extended-plan-mode#:~:text=Stage%201%3A%20Context%20Priming%E2%80%8B)
- [structured-plan-mode | Claude Code Handbook](https://nikiforovall.blog/claude-code-rules/component-reference/skills/structured-plan-mode/#:~:text=Skill%20Specification%E2%80%8B)
- [structured-plan-mode | Claude Code Handbook](https://nikiforovall.blog/claude-code-rules/component-reference/skills/structured-plan-mode/#:~:text=4.%20,md%20Progress%20Summary)
- [structured-plan-mode | Claude Code Handbook](https://nikiforovall.blog/claude-code-rules/component-reference/skills/structured-plan-mode/#:~:text=2,Status%3A%20%F0%9F%9F%A2%20In%20Progress)
- [Build a powerful team of LLM agents, to solve complicated multi step tasks](https://techcommunity.microsoft.com/blog/azure-ai-foundry-blog/build-your-dream-team-with-autogen/4157961#:~:text=The%20idea%20of%20using%20LLMs,can%20execute%20various%20complicated%20tasks)
- [Extended Plan Mode | GitHub Copilot Handbook](https://nikiforovall.blog/github-copilot-rules/guides/extended-plan-mode#:~:text=Stage%201%3A%20Context%20Priming%E2%80%8B)
- [structured-plan-mode | Claude Code Handbook](https://nikiforovall.blog/claude-code-rules/component-reference/skills/structured-plan-mode/#:~:text=2.%20%F0%9F%9F%A2%20,move)
- [Runway Workflows | Runway AI](https://runwayml.com/workflows#:~:text=Combine%20multiple%20models%2C%20modalities%20and,tasks)
- [Lumos & Luma: Illuminating Your AI Video Workflow | ReelMind](https://reelmind.ai/blog/lumos-luma-illuminating-your-ai-video-workflow#:~:text=innovation%20and%20efficiency%20in%20the,ecosystem%20for%20AI%20video%20artistry)
- [PDG node network interface](https://www.sidefx.com/docs/houdini/tops/ui.html#:~:text=Dot%20%20,work%20item%20generated%20a%20warning)
- [Using the visualization graph - GitHub Docs](https://docs.github.com/actions/managing-workflow-runs/using-the-visualization-graph#:~:text=5,Lines%20between%20jobs%20indicate%20dependencies)
- [Project dependencies – Linear Docs](https://linear.app/docs/project-dependencies#:~:text=When%20a%20dependency%20relationship%20has,fields)
- [Dependencies on monday.com – Support](https://support.monday.com/hc/en-us/articles/360007402599-Dependencies-on-monday-com#:~:text=only%20this%2C%20but%20when%20dragging,%F0%9F%99%8C)
- [material design - stepper wizard, optional step, skip and next button? - User Experience Stack Exchange](https://ux.stackexchange.com/questions/126493/stepper-wizard-optional-step-skip-and-next-button#:~:text=If%20a%20step%20is%20optional,of%20options%20you%20can%20do)
- [Project dependencies – Linear Docs](https://linear.app/docs/project-dependencies#:~:text=Dependency%20line%20color%E2%81%A0)
