# Project Config Detection Benchmark
# Evaluates models on auto-detecting project metadata from a screenplay.
# Mirrors CineForge's project_config_v1 module — config detection step.
#
# Usage: cd benchmarks && promptfoo eval -c tasks/config-detection.yaml --no-cache -j 3
# View:  promptfoo view

description: "Project config detection from screenplay — The Mariner"

defaultTest:
  options:
    provider: anthropic:messages:claude-opus-4-6

prompts:
  - file://../prompts/config-detection.txt

providers:
  # --- Cheap tier ---
  - id: openai:gpt-4.1-nano
    label: "GPT-4.1 Nano"
    config:
      temperature: 0
      max_tokens: 4096
      response_format: { type: "json_object" }

  - id: openai:gpt-4.1-mini
    label: "GPT-4.1 Mini"
    config:
      temperature: 0
      max_tokens: 4096
      response_format: { type: "json_object" }

  - id: openai:gpt-5-nano
    label: "GPT-5 Nano"
    config:
      temperature: 0
      max_tokens: 4096
      response_format: { type: "json_object" }

  - id: openai:gpt-5-mini
    label: "GPT-5 Mini"
    config:
      temperature: 0
      max_tokens: 4096
      response_format: { type: "json_object" }

  - id: google:gemini-2.5-flash-lite
    label: "Gemini 2.5 Flash Lite"
    config:
      temperature: 0
      maxOutputTokens: 16384

  # --- Mid tier ---
  - id: openai:gpt-4.1
    label: "GPT-4.1"
    config:
      temperature: 0
      max_tokens: 4096
      response_format: { type: "json_object" }

  - id: anthropic:messages:claude-haiku-4-5-20251001
    label: "Claude Haiku 4.5"
    config:
      temperature: 0
      max_tokens: 4096

  - id: anthropic:messages:claude-sonnet-4-5-20250929
    label: "Claude Sonnet 4.5"
    config:
      temperature: 0
      max_tokens: 4096

  - id: anthropic:messages:claude-sonnet-4-6
    label: "Claude Sonnet 4.6"
    config:
      temperature: 0
      max_tokens: 4096

  - id: google:gemini-2.5-flash
    label: "Gemini 2.5 Flash"
    config:
      temperature: 0
      maxOutputTokens: 16384

  - id: google:gemini-3-flash-preview
    label: "Gemini 3 Flash"
    config:
      temperature: 0
      maxOutputTokens: 16384

  # --- SOTA tier ---
  - id: openai:gpt-5.2
    label: "GPT-5.2"
    config:
      temperature: 0
      max_tokens: 4096
      response_format: { type: "json_object" }

  - id: anthropic:messages:claude-opus-4-6
    label: "Claude Opus 4.6"
    config:
      temperature: 0
      max_tokens: 4096

  - id: google:gemini-2.5-pro
    label: "Gemini 2.5 Pro"
    config:
      temperature: 0
      maxOutputTokens: 16384

  - id: google:gemini-3-pro-preview
    label: "Gemini 3 Pro"
    config:
      temperature: 0
      maxOutputTokens: 16384

tests:
  # Single test case — config detection analyzes the full screenplay
  - vars:
      screenplay: file://../input/the-mariner.md
      golden_path: golden/the-mariner-config.json
    assert:
      - type: python
        value: file://../scorers/config_detection_scorer.py
      - type: llm-rubric
        value: |
          Evaluate the project config detection for "The Mariner" screenplay.

          The model was asked to detect 10 project metadata fields from the screenplay text.

          IMPORTANT CONTEXT: "The Mariner" is a COMPLETE SHORT FILM screenplay (~10-15 formatted
          pages, FADE IN to CUT TO BLACK, with a full narrative arc). It is NOT a feature film
          or an excerpt from a longer work.

          Key quality criteria:
          1. Title: Did it correctly identify "The Mariner" as the title?
          2. Format: Did it correctly identify this as a short film? (Models calling it a "feature film" are wrong — it's a complete ~12-page screenplay with beginning, middle, and end)
          3. Genre: Does it include action, crime, and/or thriller? (It's a vigilante crime-action story with dark comedy elements)
          4. Tone: Does it capture multiple tonal qualities? (dark, gritty, violent, intense, suspenseful, fast-paced, bleak, dystopian are all valid)
          5. Duration: Is the estimate reasonable (10-25 minutes for a ~10-15 page short film)? Models estimating 85+ minutes are wrong.
          6. Primary characters: Does it identify The Mariner and Rose as leads?
          7. Supporting characters: Does it list Dad, Salvatori, Vinnie, Mikey, Carlos, Rosco among supporting cast?
          8. Location count: Is the estimate reasonable (5-20 distinct locations)?
          9. Locations summary: Does it mention the Ruddy & Greene Building as the primary setting?
          10. Target audience: Does it correctly identify this as mature/adult content?
          11. Confidence calibration: Are confidence scores reasonable and well-calibrated?
          12. Rationale quality: Are rationales grounded in actual screenplay evidence?

          Score strictly: the metadata should be accurate and evidence-based, not guessed.
