# Location Extraction Benchmark
# Evaluates models on extracting structured location data from a screenplay.
# Mirrors CineForge's location_bible_v1 module.
#
# Usage: cd benchmarks && promptfoo eval -c tasks/location-extraction.yaml --no-cache -j 3
# View:  promptfoo view

description: "Location extraction from screenplay — The Mariner"

defaultTest:
  options:
    provider: anthropic:messages:claude-opus-4-6

prompts:
  - file://../prompts/location-extraction.txt

providers:
  # --- Cheap tier ---
  - id: openai:gpt-4.1-nano
    label: "GPT-4.1 Nano"
    config:
      temperature: 0
      max_tokens: 4096
      response_format: { type: "json_object" }

  - id: openai:gpt-4.1-mini
    label: "GPT-4.1 Mini"
    config:
      temperature: 0
      max_tokens: 4096
      response_format: { type: "json_object" }

  - id: google:gemini-2.5-flash-lite
    label: "Gemini 2.5 Flash Lite"
    config:
      temperature: 0
      maxOutputTokens: 16384

  # --- Mid tier ---
  - id: openai:gpt-4.1
    label: "GPT-4.1"
    config:
      temperature: 0
      max_tokens: 4096
      response_format: { type: "json_object" }

  - id: anthropic:messages:claude-haiku-4-5-20251001
    label: "Claude Haiku 4.5"
    config:
      temperature: 0
      max_tokens: 4096

  - id: anthropic:messages:claude-sonnet-4-5-20250929
    label: "Claude Sonnet 4.5"
    config:
      temperature: 0
      max_tokens: 4096

  - id: google:gemini-2.5-flash
    label: "Gemini 2.5 Flash"
    config:
      temperature: 0
      maxOutputTokens: 16384

  - id: google:gemini-3-flash-preview
    label: "Gemini 3 Flash"
    config:
      temperature: 0
      maxOutputTokens: 16384

  # --- SOTA tier ---
  - id: openai:gpt-5.2
    label: "GPT-5.2"
    config:
      temperature: 0
      max_tokens: 4096
      response_format: { type: "json_object" }

  - id: anthropic:messages:claude-opus-4-6
    label: "Claude Opus 4.6"
    config:
      temperature: 0
      max_tokens: 4096

  - id: google:gemini-2.5-pro
    label: "Gemini 2.5 Pro"
    config:
      temperature: 0
      maxOutputTokens: 16384

  - id: google:gemini-3-pro-preview
    label: "Gemini 3 Pro"
    config:
      temperature: 0
      maxOutputTokens: 16384

tests:
  # Test 1: RUDDY & GREENE BUILDING — main setting, multi-floor, most complex
  - vars:
      location_name: "RUDDY & GREENE BUILDING"
      screenplay: file://../input/the-mariner.md
      golden_path: golden/the-mariner-locations.json
    assert:
      - type: python
        value: file://../scorers/bible_extraction_scorer.py
      - type: llm-rubric
        value: |
          Evaluate the location extraction for RUDDY & GREENE BUILDING from "The Mariner."

          Key quality criteria:
          1. Does it describe the building as a classic 15-story sandstone structure, now decayed?
          2. Does it mention bullet holes, graffiti, and the contrast with its former beauty?
          3. Does it identify the building as the main setting housing Salvatori's criminal operation?
          4. Does it note the multi-floor structure (elevator, stairwells, floors 11-15)?
          5. Does it capture the vertical journey as a narrative device?
          6. Does it list relevant scene headings (front, rear, elevator)?
          7. Is the description grounded in actual screenplay text?

          Score strictly: the building is the central location and a good extraction captures both physical detail and narrative function.

  # Test 2: 15TH FLOOR — Salvatori's office, climactic location
  - vars:
      location_name: "15TH FLOOR"
      screenplay: file://../input/the-mariner.md
      golden_path: golden/the-mariner-locations.json
    assert:
      - type: python
        value: file://../scorers/bible_extraction_scorer.py
      - type: llm-rubric
        value: |
          Evaluate the location extraction for the 15TH FLOOR from "The Mariner."

          Key quality criteria:
          1. Does it describe the luxurious, dimly lit office atmosphere?
          2. Does it mention specific details: wooden desk, Persian rug, leather-bound books, fine booze?
          3. Does it identify this as Salvatori's personal headquarters?
          4. Does it note this is where the climactic confrontation occurs?
          5. Does it mention evidence of earlier violence (pooled blood, gouged walls)?
          6. Does it capture the contrast between luxury and violence?
          7. Does it note the still-burning cigar and melting ice as environmental storytelling?

          Score strictly: this is the climactic location where the story's revelations happen.

  # Test 3: COASTLINE — flashback, dual presentation, thematic heart
  - vars:
      location_name: "COASTLINE"
      screenplay: file://../input/the-mariner.md
      golden_path: golden/the-mariner-locations.json
    assert:
      - type: python
        value: file://../scorers/bible_extraction_scorer.py
      - type: llm-rubric
        value: |
          Evaluate the location extraction for the COASTLINE from "The Mariner."

          Key quality criteria:
          1. Does it identify this as a flashback-only location?
          2. Does it capture the DUAL PRESENTATION — idealized vs. dark versions?
          3. Idealized version: golden light, warm teaching moment, father-son bonding?
          4. Dark version: beer bottles, shivering child, drunk/mocking father?
          5. Does it connect this to Mariner's false memories about his father?
          6. Does it note the thematic significance — memory vs. truth?
          7. Is evidence grounded in actual screenplay descriptions?

          Score strictly: the dual presentation is the screenplay's most important narrative device and MUST be captured.
